# HuggingFace NLP course ðŸ¤—

Notes on the course from [HuggingFace][1]

## General

* Transfer learning (TL) leverages knowledge of model A on task 1 for model B on task 2 (by initializing model B with the weights of model A)
* >TL drops the head of the pre-trained model, but keeps its body
* In NLP, predicting the next word and guessing random masked word are common pre-training tasks

## Misc

* Chose eu-west3 (Paris) or eu-north-1 (Stockholm) for the lowest CO2 emissions
* Use well-researched random search instead of grid search for hyperparameter tuning

## Transformers

### Architecture

#### Encoders

* Encoder transforms text into numerical representation (embeddings)
* Embeddings take into account context of words (via self-attention)
* Encoders are bi-directional (can access context on left _and_ right), auto-regressive (can produce new output taking into account previous input until stop sequence is reached) and good at extracting meaningful information (natural language understanding)
* Encoders are used for sequence classification (i.e. sentiment analysis), question answering, masked language modeling
* Examples: bert, roberta

#### Decoders

* Decoders are unidirectional (can only access context to their right _OR_ left)
* Decoders are great a generating sequence (natural language generation)
* Examples: GPT-2

#### Encoder-Decoders

* Decoder uses output for encoder as input
* Encoder understands entire input sequence and helps decoder to generate new words
* Further output of decoder is produced by taking previous output into account, as well as embeddings generated by encoder
* Encoders-Decoders are used for sequence-to-sequence modeling
* Examples: BART
* You can load an encoder and decoder model into an Encoder-Decoders model

### Package

* Pipeline is most high-level function available and can be used for tasks such as
    - sentiment-analysis
    - zero-shot-classification (providing your own labels)
    - text-generation
    - text completion (mask filling)
    - named-entity recognition (ner)
    - question-answering
    - translation
* Pipeline can be used with any model suited to the task you want to perform
* Pipeline consists of 3 stages: tokenize, model, postprocess
* AutoTokenizer loads correct tokenizer for model
* AutoModel class only loads model without pre-training head (which can't be used for tasks directly) (use AutoModelFor<TASK> class instead)
* During postprocessing logits are transformed into probabilities by calling SoftMax on them (use `model.config.id2label` to get them)


[1]: https://huggingface.co/learn/nlp-course/chapter1/1